---
title: Unicode y encodings
description: Unicode y encodings
author: eiximenis

date: 2018-10-01T09:40:40+00:00
geeks_url: /?p=2189
geeks_ms_views:
  - 715
categories:
  - .net
tags:
  - unicode

---
Uno de los conceptos que hoy en d칤a siguen causando m치s confusi칩n es el de Unicode y sus distintos tipos de codificaci칩n. Pero... 쯤u칠 es realmente Unicode? Para ello, d칠jame que remonte unos cuantos a침os atr치s...
  
**El inicio: ASCII**
  
Los ordenadores los inventaron los americanos y como suele ocurrir se preocuparon de lo suyo: que un ordenador pudiese presentar textos en su idioma. Tampoco hay tantos caracteres en el ingl칠s: las veinte y poco letras (en may칰sculas y min칰sculas), los s칤mbolos de puntuaci칩n, par칠ntesis, operaciones matem치ticas y poco m치s. En total eran menos de 128 car치cteres. Genial, porque esos espacios que sobraban se pod칤an aprovechar para colocar otros caracteres que no tienen representaci칩n gr치fica pero que eran (y son) necesarios para controlar el teletipo: retornos de carro, saltos de l칤nea y similares. Hab칤a nacido el [c칩digo ASCII][1].
  
<!--more-->


  
Aqu칤 ya hay confusi칩n: mucha gente cree, err칩neamente que el c칩digo ASCII es un c칩digo de 8 bits por caracter, pero es falso. ASCII es un c칩digo de 128 valores y por lo tanto se usan 7 bits por cada caracter (00-7F). Ten presente,**y esa es la clave de todo el l칤o que los ordenadores NO entienden de car치cteres. Solo de bytes**. Los car치cteres son cosa de humanos y el meollo del asunto es &#8220;como pasamos de un churro de M bytes a una cadena de N car치cteres&#8221;.
  
Ahora bien dado que los ordenadores trabajan habitualmente con bytes, y un byte son 8 bits con ASCII nos sobra uno. Puede parecer que un bit no da para mucho, pero bueno... 춰nos permite doblar el n칰mero de caracteres! Obvio: si en lugar de 7 bits usamos 8 bits por caracter, pasamos de 128 a 256 valores. Y la pregunta est치 clara:**qu칠 hacemos con los 128 valores restantes?** Pues, literalmente, cada cual hizo lo que le di칩 la gana. Salieron as칤 los c칩digos &#8220;ASCII extendidos&#8221; (los que mucha gente cree que son ASCII). Pero f칤jate que hablo en plural! 춰Es que hay m치s de uno! Como te digo cada cual aprovech칩 para meter sus propios 128 caracteres adicionales. En windows tenemos el comando &#8220;chcp&#8221; que nos permite cambiar &#8220;la p치gina de c칩digos del terminal&#8221; para que interprete los bytes (de entrada y salida) bas치ndose en uno de estos c칩digos ASCII extendidos. As칤, bajo p치ginas de c칩digo distintas el mismo fichero (churro de bytes) lo ver치s con contenido diferente!
  
**El primer l칤o: SBCS y DBCS**
  
Como digo todo el mundo se puso a redefinir los 128 car치cteres adicionales que ASCII nos dejaba libre y por supuesto los japoneses no quisieron ser menos. Pero ellos se dieron de bruces con un problema: 춰tienen m치s de 128 car치cteres! As칤 que ni con los 128 car치cteres libres les bastaba.
  
쯃a soluci칩n? Si un byte no te da para almacenar todos tus car치cteres... 춰usa dos! Eso te da la capacidad te칩rica de 65536 car치cteres distintos, pero la realidad no es tan sencilla: las p치ginas de c칩digos deben ser compatibles en los 127 primeros car치cteres. Eso significa que un fichero que tenga solo bytes en el rango 0x00-0x7f se debe ver bien en todas las p치ginas de c칩digos. Por lo tanto se inventaron una codificaci칩n donde &#8220;algunos&#8221; de los valores por encima de 0x7f indicaban que el siguiente byte tambi칠n formaba parte del car치cter. As칤 nac칤an las codificaci칩nes DBCS (Double Byte Character Set). Bajo DBCS no tenemos 65536 car치cteres distintos, si no bastantes menos y no todos los car치cteres ocupan dos bytes, pero bueno, a los japoneses ya les bastaba. Por contraposici칩n a todas las otras p치ginas de c칩digos de idiomas mortales, que les bastaba con un byte por caracter se las conoci칩 como SBCS (Single Byte Character Set).
  
**A poner orden: Unicode**
  
Por supuesto hay m치s de una p치gina de c칩digos DBCS: los chinos, los coreanos y dem치s no quisieron ser menos y tambi칠n se crearon sus propias p치ginas de c칩digos con sus car치cteres. Eso se nos estaba yendo de las manos.
  
Por ello al final se decidi칩**crear un est치ndard** para unificar todas las p치ginas de c칩digos en una. Se usar칤an 2 bytes por car치cter, lo que nos da, ahora s칤, 65536 car치cteres diferentes que nos deben bastar para todas las lenguas. Se cre칩 un consorcio, el [consorcio Unicode][2] y se les pag칩 para que definieran una lista con los 65536 valores y as칤 que nuestra vida fuese mucho m치s f치cil.
  
La gente de Unicode ten칤an dos virtudes: la primera es que tontos no eran, y para evitar follones, los primeros 127 valores Unicode son el c칩digo ASCII (en caso contrario se hubiese armado un foll칩n que r칤ete tu del [efecto 2000][3]). La segunda virtud es que eran hijos de abogados y por eso encontraron una manera de complicarlo todo hasta el l칤mite para que los inform치ticos tuvi칠ramos algo de que vivir y poder escribir posts como este.
  
**La complejidad de Unicode**
  
Mucha gente se cree que Unicode es un est치ndard que define (un m치ximo de) 65536 car치cteres (2 bytes por car치cter) y que se trata de una lista numerada donde a cada valor le corresponde un car치cter. Esta visi칩n tiene dos problemas: el primero es que es una simplificaci칩n muy burda y el segundo que es falsa.
  
Es falsa porque simple y llanamente**Unicode tiene m치s de 65536 car치cteres**(la 칰ltima versi칩n de Unicode, la 11.0, tiene 137374 car치cteres) y es una simplificaci칩n muy burda porque en Unicode es posible definir car치cteres como combinaciones de otros car치cteres (parece una idea muy loca pero tiene su utilidad). Mucha gente sigue creyendo que Unicode define un m치ximo de 65536 car치cteres**porque empez칩 as칤**, pero m치s adelante se vi칩 que eso era insuficiente y se modific칩 el est치ndard. A cada nueva versi칩n se le han ido agregando car치cteres.
  
A partir de ahora voy a ponerme un poco m치s estricto y usar칠 la siguiente nomenclatura:

  * _code point_ para referirme a un car치cter Unicode.
  * _code points BMP_ (Basic Multilingual Plane) para referirme a aquellos_code points_ en el rango U+0000 &#8211; U+FFFF.
  * _code points no-BMP****_para referirme a aquellos_code points_fuera del rango BMP (a partir de U+10000 para adelante)

Para**codificar un _code point_Unicode necesitamos 4 bytes** (aunque no usamos todos los 32 bits porque hay menos de 2<sup>32</sup> car치cteres Unicode). Si**para cada _code point_usamos siempre los 32 bits, tenemos lo que se llama UTF32.** UTF32 es la codificaci칩n m치s simple... y la menos usada. Simple porque a cada car치cter Unicode le corresponde un 칰nico valor de 32 bits, y la menos usada porque ocupa gran cantidad de espacio. Observa que usando UTF32 cada car치cter ocupa 32 bits, incluso aquellos con c칩digos bajos (p. ej. ASCII), que son los car치cteres m치s usados. As칤 un texto en UTF32 que tenga 10 car치cters ocupar치 40 bytes, con independencia de que esos 10 car치cteres sean 10 &#8216;A&#8217;s o bien 10 car치cteres raros (cre칠me: hay car치cteres muy raros en Unicode).
  
Es por ello que, para evitar este uso de espacio, se usan otras codificaciones como UTF7, UTF8 y UTF16. Que como habr치s adivinado usan 7 bits, 8 bits o 16 para cada... 쯣ara cada qu칠? Est치 claro que para cada _code point_Unicode no (porque no cabe). **Cuando usamos estas otras codificaciones debemos entender el concepto de <span style="text-decoration: underline;"><em>code unit</em></span>**. As칤 UTF7 usa _code units_ de 7 bits, UTF8 las usa de 8 bits y UTF16 de 16 bits.**Y un _code point_ (car치cter)Unicode se compone de una o m치s _code units_ combinadas**. As칤 en UTF8 hay algunos car치cteres Unicode (p. ej. los ASCII) que ocupan solo 1 byte, hay otros que ocupan dos y los hay que ocupan 3 y hasta 4 bytes. As칤 un texto codificado en UTF8 siempre ocupa menos que el mismo texto en UTF32. Algo parecido ocurre con UTF16 (aunque en este caso, cada_code unit_ ocupa 2 bytes, por lo que el ahorro de espacio no es tan sensible).
  
**Unicode y .NET**
  
Si has estado leyendo el post, quiz치 te haya sorprendido que Unicode defina m치s de 65536 car치cteres. Ya que 쯡o se supone que .NET soporta Unicode? Pero un_char_ de .NET son 16 bits. Es decir**no hay espacio en un char de .NET para representar la totalidad de_code points_ de Unicode**. 쮼ntonces... qu칠 ocurre?
  
Pues la respuesta**es que char NO ES un _code point_(car치cter_)_de Unicode. <span style="text-decoration: underline;">Char es un<em>code unit</em>de UTF16</span>**. Y s칤, las cadenas de .NET no son secuencias de car치cteres Unicode: son secuencias de_code units_ de UTF16. Por lo tanto**para algunos car치cteres de Unicode se requiere m치s de un char de .NET para representarlos**. Es por eso que se dice siempre que **.NET est치 basado en UTF16**. Cuanto antes asimiles esta idea menos problemas tendr치s.
  
Pero... como podemos representar mediante .NET un car치cter Unicode que requiera m치s de un_code unit_ en UTF16. P. ej. el car치cter**U+1000A** (65546 decimal) se representa de la siguiente manera:

  1. 0x0001000A en UTF32 (recuerda en UTF32 cada valor es un_code point_ de Unicode)
  2. 0xD800,0xDC0A en UTF16 (dos_code units_)
  3. 0xF0, 0x90, 0x80,0x8A en UTF8 (cuatro_code units_)

Observa que**no podemos crear una cadena que contenga este car치cter &#8220;tal cual&#8221;:**

<pre class="EnlighterJSRAW" data-enlighter-language="csharp">var impossible = "\u0001000A";</pre>

Esto funciona, pero**_impossible_ no contiene el car치cter U+1000A**, si no que es una cadena con cinco car치cteres (0x1, 0x30 (&#8216;0&#8217;), 0x30, 0x30, 0x65 (&#8216;A&#8217;)). La sintaxis &#8216;\uxxxx&#8217; define un car치cter cuyo c칩digo es &#8220;xxxx&#8221; pasado a hexadecimal. Siempre son cuatro valores (0x0000 &#8211; 0xffff), as칤 que lo que sigue se consideran car치cteres normales. As칤, pues es como si hubieramos hecho &#8220;\u0001&#8221; + &#8220;000A&#8221;. Por supuesto si hago_impossible = &#8220;\u1000A&#8221;_ (sin los cero iniciales), la cadena tiene dos car치cteres (el primero es 0x1000 y el segundo una letra A).
  
Para representar una car치cter Unicode como U+1000A, debo partir de su representaci칩n en UTF32 y usar el m칠todo[ConvertFromUtf32][4] de la clase char. Este m칠todo**toma un int (32 bits) con el c칩digo UTF32 y devuelve UNA CADENA que contiene el car치cter Unicode**:

<pre class="EnlighterJSRAW" data-enlighter-language="null">var possible = char.ConvertFromUtf32(0x1000A);
foreach (var x in possible)
{
    Console.Write("({0:X4})", (int)x);
}</pre>

La salida de este c칩digo es (D800)(DC0A) que son los dos_code units_en UTF16. **Es por eso que el m칠todo devuelve una string: hay car치cteres Unicode (_code points_) que requieren m치s de un car치cter .NET (_code unit_ de UTF16) para representarse**. En este caso la propiedad Length de_possible_ es 2.
  
**No hay en .NET una API orientada a_code points_ (car치cteres) de Unicode que nos permita saber cuantos car치cteres Unicode tiene una secuencia de bytes.**La API de .NET (las clases String y char) est치 orientada a UTF16. S칤 que tenemos herramientas para saber si un char de .NET es directamente un_code point_ de Unicode o bien se trata de un_code unit_ de UTF16 que forma parte de la representaci칩n de un_code point_de Unicode que toma m치s de un_code unit_ en UTF16. El m칠todo_IsSurrogate_ (est치tico de char) nos dice si un_code unit_ de UTF16 (char en .NET) es un_surrogate_ (쯥ustituto?) o no (m치s sobre_surrogates_ en breve).
  
Tener una API orientada a UTF16 tiene un problema:**dada una cadena Unicode, representada como String en .NET, no podemos f치cilmente acceder al i-칠simo car치cter Unicode**. Ya, es cierto que las cadenas tienen 칤ndice, pero este 칤ndice nos devuelve el i-칠simo_code unit_ de UTF16, no el i-칠simo_code point_ de la cadena Unicode. Por lo tanto debemos recorrer toda la cadena y parsearla.
  
**Surrogates en Unicode y UTF16**
  
Un_surrogate_ es un_code point_ (es decir car치cter Unicode) en el rangoU+D800 &#8211; U+DFFF (hay pues 2048 surrogates), que se dividen en dos grupos:

  * U+D800 &#8211; U+DBFF (1024 surrogates) conocidos como_high surrogates_
  * U+DC00 &#8211; U+DFFF (los otros 1024) conocidos como_low surrogates_

En UTF16 aquellos_code units no BMP_ (a partir de U+10000) se representan mediante un par de_surrogates_, siempre un_high surrogate_ primero y un_low surrogate_. Observa el ejemplo anterior en el que U+1000A se representaba mediante U+D800 (el primero de los_high surrogates_) seguido de U+DC0A (en el rango de los_low surrogates_).
  
Y ahora algo importante:**los_surrogates_estan reservados para su uso en UTF16**. Fuera de una cadena en UTF16 no se pueden usar y adem치s siempre deben aparecer en pares (_high, low_). Una cadena Unicode que contenga un_surrogate_ sin su pareja es inv치lida. Y, insisto: s칩lo en UTF16. Eso significa que**en UTF8 no se necesitan_surrogates_**_._A pesar de qu칠 t칠cnicamente es posible codificar una cadena Unicode que contenga_surrogates_ en UTF8, esta cadena**deber칤a ser considera inv치lida**. Un parser de UTF8 puede negarse a codificar/decodificar dicha cadena: est치 fuera del est치ndard Unicode. Es decir,**hay secuencias de car치cteres Unicode que NO SON cadenas v치lidas Unicode**.
  
Y quiero insistir en algo que he dicho al inicio de dicha secci칩n pero que igual se te ha pasado: los_surrogates_ son_code points_, es decir son car치cteres Unicode por s칤 mismos, a los que el est치ndard da un trato especial (en este caso formar parte de una secuencia UTF16).
  
**Nota:**UTF8 no necesita_surrogates_porque usa otro mecanismo para codificar los_code points_de Unicode (en funci칩n de si determinados bits de cada byte est치n establecidos o no, se sabe si este byte forma parte de un_code point_que ocupa m치s de un byte).
  
Vale... ya casi hemos diseccionado el tema, pero nos queda una cosilla m치s para tener el puzle m치s o menos completo...
  
**UCS-2, UCS-4**
  
[UCS][5] significa &#8220;Universal Character Set&#8221; y es un est치ndard que define una lista de car치cteres y su c칩digo. As칤 UCS-2 define una lista de car치cteres que ocupan cada uno 2 bytes y UCS-4 lo extiende a 4 bytes.
  
쯊e suena muy parecido a Unicode, eso? 춰Es que lo es! Por suerte, y a pesar de que Unicode y UCS empezaron distinto, se han coordinado**y tenemos una 칰nica lista de_code points_.**UCS-2 define todos los_code points_ del rango BMP (0x0000-0xffff) mientras que UCS-4 define todos esos y los siguientes (hasta 31 bits). Lo importante es que los c칩digos son los mismos que Unicode.
  
Entonces... en que se diferencia UCS de Unicode? 쯏 p. ej. que diferencia hay entre UCS-2 y UTF16 o entre UCS-4 y UTF32?
  
Entre Unicode y UCS la diferencia est치 que el segundo es una mera lista de_code points_ con su c칩digo y Unicode va mucho m치s all치: define reglas de codificaci칩n (p. ej._surrogates_) pero tambi칠n de normalizaci칩n de cadenas,_collation_ y dem치s. Es decir Unicode no es (s칩lo) una lista de_code points_, es todo un est치ndard que define como trabajar con ellos. Todo eso queda fuera del alcance de UCS.
  
Entre UTF16 y UCS-2, ambos usan un tama침o de_code unit_ de 16 bits. La diferencia est치 en que el primero puede representar todo el rango de_code points_ de UCS-4, mientras que el segundo, no (el segundo solo puede representar_code points_ en el rango BMP y en UCS el concepto de_surrogate_ no existe).
  
Entre UTF32 y UCS-4, ambos usan un tama침o de_code unit_ de 32 bits. La diferencia est치 en que el primero es, de hecho, un subconjunto del segundo. En UCS-4 el rango de car치cteres v치lidos va de 0x0 &#8211; 0x7fffffff (31 bits), mientras que UTF32 al ser una representaci칩n de Unicode est치 limitado por el rango de car치cteres definido en Unicode.
  
Bueno... lo dejamos aqu칤. Espero que este post os haya ayudado a aclarar algunos conceptos si es que los ten칤ais confundidos 游뗵

 [1]: https://en.wikipedia.org/wiki/ASCII
 [2]: https://es.wikipedia.org/wiki/Consorcio_Unicode
 [3]: https://es.wikipedia.org/wiki/Problema_del_a%C3%B1o_2000
 [4]: https://docs.microsoft.com/en-us/dotnet/api/system.char.convertfromutf32?view=netframework-4.7.2
 [5]: https://es.wikipedia.org/wiki/ISO/IEC_10646